{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the World Bank's Twitter Feed, Judy Yang, DAT10 Project\n",
    "### Part 4. Text Analysis\n",
    "\n",
    "#### Tokenization, word counts\n",
    "#### Prediction Linear Regression\n",
    "#### Topic Modelling\n",
    "#### Predict High/Low Popular Tweets\n",
    "#### Term Frequency\n",
    "#### Sentiment Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/judyyang/Documents/GA_DS_course/Final_Project/notebooks'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import operator \n",
    "import preprocess\n",
    "from collections import Counter\n",
    "#from textblob import TextBlob\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "#% sign \n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim import corpora, models, similarities\n",
    "from collections import defaultdict\n",
    "from sklearn import metrics\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from ttp import ttp\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows= 50\n",
    "pd.options.display.width= 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create excel to save outputs from this notebook\n",
    "writer = ExcelWriter('./data/Project04_outputs.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wb = pd.read_pickle('./data/WorldBank_all_processed_17feb_2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71287, 21)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb = wb[(wb.is_RT==0)]\n",
    "wb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wb=wb[wb.user_screen_name==\"WorldBank\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wb['favorite75']=np.where(wb.favorite_count>=49, 1, 0)\n",
    "wb['retweet75']=np.where(wb.retweet_count>=70, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    71287.000000\n",
       "mean         0.014238\n",
       "std          0.118472\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          1.000000\n",
       "Name: favorite75, dtype: float64"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb.favorite75.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    71287.000000\n",
       "mean         0.014308\n",
       "std          0.118760\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          1.000000\n",
       "Name: retweet75, dtype: float64"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb.retweet75.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wb = wb.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Save the tweets with the top retweets and favorite counts\n",
    "top_retweets=wb[(wb.retweet_count>=300)].sort_values(\"retweet_count\", ascending=False)\n",
    "top_retweets.to_excel(writer,'top_retweets')\n",
    "\n",
    "top_fav=wb[(wb.favorite_count>=300)].sort_values(\"favorite_count\", ascending=False)\n",
    "top_fav.to_excel(writer,'top_favs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions: what determines high retweets and favorites?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "\n",
    "How much of retweet or favorite responses can be explained by non-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise :  define a function that accepts a list of features and returns testing RMSE\n",
    "def train_test_rmse(cols):\n",
    "   \n",
    "    X = wb[cols]\n",
    "    y= wb.retweet75\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99)\n",
    "    \n",
    "    # instantiate and fit\n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(X_train, y_train)\n",
    "\n",
    "    # print the coefficients\n",
    "    true=y_test\n",
    "    pred=linreg.predict(X_test)\n",
    "    rmse=np.sqrt(metrics.mean_squared_error(true, pred))\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.122310737948\n",
      "0.122376569849\n",
      "0.122311457742\n"
     ]
    }
   ],
   "source": [
    "# compare different sets of features\n",
    "#1) has_at \n",
    "print train_test_rmse(['has_at'])\n",
    "\n",
    "#2) has a hashtag\n",
    "print train_test_rmse(['has_ht'])\n",
    "\n",
    "#3) has_at, has a hashtag, has a link, is a RT\n",
    "print train_test_rmse(['has_at', 'has_ht', 'has_link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing testing RMSE with null RMSE\n",
    "Null RMSE is the RMSE that could be achieved by always predicting the mean response value. It is a benchmark against which you may want to measure your regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.122310737948\n"
     ]
    }
   ],
   "source": [
    "print train_test_rmse(['has_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split the new DataFrame into training and testing sets\n",
    "wb['favorite0']=np.where(wb.favorite_count>0, 1, 0)\n",
    "wb['retweet0']=np.where(wb.retweet_count>0, 1, 0)\n",
    "\n",
    "X=wb.has_at\n",
    "y=wb.retweet0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4531526200810515"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split X and y into training and testing sets\n",
    "# create a NumPy array with the same shape as y_test\n",
    "y_null = np.zeros_like(y_test, dtype=float)\n",
    "\n",
    "# fill the array with the mean value of y_test\n",
    "y_null.fill(y_test.mean())\n",
    "y_null\n",
    "# compute null RMSE\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_null))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stoplist = set(CountVectorizer(stop_words='english').get_stop_words() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=wb.text_clean\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist] for document in list(X)]\n",
    "\n",
    "# count up the frequency of each word\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "     for token in text:\n",
    "         frequency[token] += 1    \n",
    "        \n",
    "# (2) remove words that only occur a small number of times, fixing a feature space that's needlessly big.\n",
    "# once in the whole corpus, not just once in a single document\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=10, alpha = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'0.017*credit + 0.016*new + 0.012*business + 0.010*online + 0.010*close + 0.009*week + 0.009*soil + 0.009*applications + 0.008*comment + 0.008*jobs', u'0.036*join + 0.035*wbg + 0.027*live + 0.024*climate + 0.020*get2equal + 0.017*change + 0.015*watch + 0.012*learning + 0.012*today + 0.011*corruption', u'0.033*worldbank + 0.029*world + 0.020*bank + 0.018*report + 0.017*new + 0.014*countries + 0.014*financialinclusion + 0.011*mt + 0.011*read + 0.010*global', u'0.027*finance + 0.017*farmers + 0.015*risk + 0.013*smes + 0.009*helping + 0.009*disasters + 0.009*development + 0.009*natural + 0.008*disaster + 0.008*resilience', u'0.031*women + 0.023*financial + 0.021*financialinclusion + 0.020*access + 0.019*finaccess + 0.014*india + 0.012*services + 0.011*help + 0.011*gender + 0.011*agriculture', u'0.026*apply + 0.025*webinar + 0.017*sign + 0.016*land + 0.015*solutions + 0.014*innovation + 0.011*prevent + 0.011*ecourse + 0.009*job + 0.009*youth', u'0.029*ecourse + 0.016*food + 0.011*course + 0.011*need + 0.010*getting + 0.009*climatesmart + 0.009*urban + 0.009*worldbank + 0.009*cities + 0.009*emissions', u'0.021*poverty + 0.021*farmers + 0.020*education + 0.019*climatechange + 0.018*women + 0.018*help + 0.012*girls + 0.011*foodsecurity + 0.011*agriculture + 0.009*management', u'0.032*ifc + 0.023*org + 0.022*private + 0.022*sector + 0.022*worldbank + 0.018*gender + 0.018*violence + 0.015*new + 0.015*development + 0.015*data', u'0.019*remittances + 0.016*16days + 0.016*energy + 0.015*entrepreneurs + 0.012*new + 0.011*use + 0.010*like + 0.009*follow + 0.009*available + 0.009*data']\n"
     ]
    }
   ],
   "source": [
    "print lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Topic model only the top retweet and favorite tweets **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'0.016*africa + 0.014*help + 0.014*women + 0.013*poor + 0.009*poverty + 0.009*violence + 0.009*cities + 0.009*climate + 0.008*endpoverty + 0.008*new',\n",
       " u'0.018*women + 0.018*poverty + 0.014*access + 0.013*wbg + 0.013*jimkim + 0.012*ppl + 0.011*energy + 0.011*climate + 0.010*countries + 0.009*global',\n",
       " u'0.028*countries + 0.017*access + 0.014*climatechange + 0.012*poverty + 0.012*wbg + 0.010*developing + 0.010*ppl + 0.010*endpoverty + 0.009*help + 0.009*people',\n",
       " u'0.037*poverty + 0.020*wbg + 0.018*jimkim + 0.015*endpoverty + 0.013*new + 0.011*world + 0.011*women + 0.011*climate + 0.011*climatechange + 0.011*report',\n",
       " u'0.019*poverty + 0.017*cop + 0.016*climate + 0.011*global + 0.011*energy + 0.011*africa + 0.011*development + 0.010*ending + 0.009*ppl + 0.009*poor',\n",
       " u'0.018*poverty + 0.013*endpoverty + 0.013*people + 0.013*opportunity + 0.012*world + 0.012*women + 0.009*education + 0.009*cop + 0.009*change + 0.009*ending',\n",
       " u'0.015*world + 0.014*poverty + 0.012*africa + 0.011*wbg + 0.010*climatechange + 0.010*cop + 0.009*global + 0.009*countries + 0.009*jimkim + 0.008*good']"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=wb[wb.retweet75==1].text_clean\n",
    "\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist] for document in list(X)]\n",
    "\n",
    "# count up the frequency of each word\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "     for token in text:\n",
    "         frequency[token] += 1 \n",
    "        \n",
    "# remove words that only occur a small number of times, fixing a feature space that's needlessly big.\n",
    "# once in the whole corpus, not just once in a single document\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=7, alpha = 'auto')\n",
    "\n",
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict retweet or favorites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the new DataFrame into training and testing sets\n",
    "wb['favorite0']=np.where(wb.favorite_count>0, 1, 0)\n",
    "wb['retweet0']=np.where(wb.retweet_count>0, 1, 0)\n",
    "\n",
    "X=wb.text_clean\n",
    "y=wb.favorite0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use CountVectorizer to create document-term matrices from X_train and X_test\n",
    "#vect = CountVectorizer()\n",
    "# remove English stop words\n",
    "vect = CountVectorizer(stop_words='english', lowercase=True)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.661766356189\n"
     ]
    }
   ],
   "source": [
    "# use Naive Bayes to predict the star rating\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# calculate accuracy\n",
    "print metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60632925597576026"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate null accuracy\n",
    "y_test_binary = np.where(y_test==1, 1, 0)\n",
    "max(y_test_binary.mean(), 1 - y_test_binary.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48856329101114554"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null RMSE\n",
    "y_null = np.zeros_like(y_test, dtype=float)\n",
    "\n",
    "# fill the array with the mean value of y_test\n",
    "y_null.fill(y_test.mean())\n",
    "y_null\n",
    "# compute null RMSE\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_null))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts a vectorizer and calculates the accuracy\n",
    "def tokenize_test(vect):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    print 'Features: ', X_train_dtm.shape[1]\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "    print 'Accuracy: ', metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  266521\n",
      "Accuracy:  0.661429693637\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  31208\n",
      "Accuracy:  0.648860958366\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(1, 3))\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ddd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-246-d6d88f2e5008>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mddd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ddd' is not defined"
     ]
    }
   ],
   "source": [
    "ddd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency and Inverse Document Frequency:\n",
    "- **What:** Computes \"relative frequency\" that a word appears in a document compared to its frequency across all documents\n",
    "- **Why:** More useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents)\n",
    "- **Notes:** Used for search engine scoring, text summarization, document clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#summarize(id_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the likelihood of favorite and retweet for each word/token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# instantiate the vectorizer\n",
    "vect = CountVectorizer(stop_words='english', analyzer='word', ngram_range=(1,2))\n",
    "\n",
    "# learn the vocabulary of ALL messages and save it\n",
    "vect.fit(wb.text_clean)\n",
    "#this is a list\n",
    "all_tokens = vect.get_feature_names()\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create separate DataFrames for high retweet and low retweet\n",
    "rhi = wb[(wb.retweet_count>50)]\n",
    "rlo = wb[wb.retweet_count<=50]\n",
    "\n",
    "fhi = wb[(wb.favorite_count>50)]\n",
    "flo = wb[wb.favorite_count<=50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create document-term matrices for retweet high and low\n",
    "rhi_dtm = vect.transform(rhi.text_clean)\n",
    "rlo_dtm = vect.transform(rlo.text_clean)\n",
    "\n",
    "# count how many times EACH token appears across ALL retweet high/low messages\n",
    "rhi_counts = np.sum(rhi_dtm.toarray(), axis=0)\n",
    "rlo_counts = np.sum(rlo_dtm.toarray(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create document-term matrices for favorite high and low\n",
    "fhi_dtm = vect.transform(fhi.text_clean)\n",
    "flo_dtm = vect.transform(flo.text_clean)\n",
    "\n",
    "# count how many times EACH token appears across ALL favorite high/low messages\n",
    "fhi_counts = np.sum(fhi_dtm.toarray(), axis=0)\n",
    "flo_counts = np.sum(flo_dtm.toarray(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a DataFrame of tokens with their separate favorite high and low counts\n",
    "token_counts = pd.DataFrame({'token':all_tokens, 'flo':flo_counts, 'fhi':fhi_counts, 'rlo':rlo_counts, 'rhi':rhi_counts})\n",
    "\n",
    "# add one to retweet/favorite high and low counts to avoid dividing by zero (in the step that follows)\n",
    "#pseudo counts\n",
    "token_counts['rlo'] = token_counts.rlo + 1\n",
    "token_counts['rhi'] = token_counts.rhi + 1\n",
    "token_counts['flo'] = token_counts.flo + 1\n",
    "token_counts['fhi'] = token_counts.fhi + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate ratio of high-low for each token\n",
    "token_counts['fav_ratio'] = token_counts.fhi / token_counts.flo\n",
    "token_counts['retweet_ratio'] = token_counts.rhi / token_counts.rlo\n",
    "\n",
    "#export to excel\n",
    "#token_counts.to_excel(writer,'likelihood_tokens_textclean50')\n",
    "#token_counts.sort_values(\"fav_ratio\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns the polarity\n",
    "def detect_sentiment(text):\n",
    "    return TextBlob(text.decode('utf-8')).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a new DataFrame column for sentiment (WARNING: SLOW!)\n",
    "wb['sentiment'] = wb.text_clean.apply(detect_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#More negative tweets get retweeted (Information sharing)\n",
    "sns.set(style=\"ticks\", context=\"talk\")\n",
    "\n",
    "# Plot tip as a function of toal bill across days\n",
    "wb['ln_RT']=np.log(wb.retweet_count+1)\n",
    "wb['ln_fav']=np.log(wb.favorite_count+1)\n",
    "g = sns.lmplot(x=\"ln_fav\", y=\"sentiment\", hue=\"year_month\", data=wb[wb.year_month==\"16-01\"], size=7)\n",
    "\n",
    "# Use more informative axis labels than are provided by default\n",
    "g.set_axis_labels(\"counts\", \"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list reviews with most positive sentiment\n",
    "wb[wb.sentiment == 1].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list reviews with most negative sentiment\n",
    "wb[wb.sentiment <0].text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gensim sentiment\n",
    "\n",
    "\n",
    "http://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "model = Word2Vec.load_word2vec_format('vectors.txt', binary=False) #C text format\n",
    "model = Word2Vec.load_word2vec_format('vectors.bin', binary=True) #C binary format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('food_words.txt', 'r') as infile:\n",
    "    food_words = infile.readlines()\n",
    "\n",
    "with open('sports_words.txt', 'r') as infile:\n",
    "    sports_words = infile.readlines()\n",
    "\n",
    "with open('weather_words.txt', 'r') as infile:\n",
    "    weather_words = infile.readlines()\n",
    "\n",
    "def getWordVecs(words):\n",
    "    vecs = []\n",
    "    for word in words:\n",
    "        word = word.replace('\\n', '')\n",
    "        try:\n",
    "            vecs.append(model[word].reshape((1,300)))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    vecs = np.concatenate(vecs)\n",
    "    return np.array(vecs, dtype='float') #TSNE expects float type values\n",
    "\n",
    "food_vecs = getWordVecs(food_words)\n",
    "sports_vecs = getWordVecs(sports_words)\n",
    "weather_vecs = getWordVecs(weather_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use TSNE and matplotlib to visualize the clusters with the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ts = TSNE(2)\n",
    "reduced_vecs = ts.fit_transform(np.concatenate((food_vecs, sports_vecs, weather_vecs)))\n",
    "\n",
    "#color points by word group to see if Word2Vec can separate them\n",
    "for i in range(len(reduced_vecs)):\n",
    "    if i < len(food_vecs):\n",
    "        #food words colored blue\n",
    "        color = 'b'\n",
    "    elif i >= len(food_vecs) and i < (len(food_vecs) + len(sports_vecs)):\n",
    "        #sports words colored red\n",
    "        color = 'r'\n",
    "    else:\n",
    "        #weather words colored green\n",
    "        color = 'g'\n",
    "    plt.plot(reduced_vecs[i,0], reduced_vecs[i,1], marker='o', color=color, markersize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
